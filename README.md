This application was developed by utilizing a machine learning model trained to identify various American Sign Language (ASL) hand gestures. It leverages OpenCV and scikit-learn for computer vision capabilities to facilitate data collection and the training of the machine learning model, while MediaPipe is employed for its advanced hand landmark estimation capabilities, encompassing over 20 distinct landmarks, thereby enhancing the accuracy of gesture recognition. 

Originally had 100 photso for each letter but realized filesize was way too big :(

![image](https://github.com/joannamooon/ASLTranslator/assets/148511962/d6d07bae-1006-4638-a09f-d31e924cea3c)



